{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"batch_mask.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PSEJG_9g0BSS"},"source":["#Setup"]},{"cell_type":"markdown","metadata":{"id":"Nbb0_5-nrFk0"},"source":["Link google drive"]},{"cell_type":"code","source":["# Link google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3GpuCK84hB6U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652305966267,"user_tz":240,"elapsed":40871,"user":{"displayName":"Timothy Renney","userId":"06599028348428035797"}},"outputId":"55e3a44c-0be4-4141-ccb5-e8cc157656da"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Install dependencies and run the setup script"],"metadata":{"id":"NnmnE3Pqye8a"}},{"cell_type":"code","source":["# Run the Custom dataset code first to create a config file for a custom dataset\n","CONFIG_PATH=\"/content/drive/MyDrive/batch-mask/data/snake-session/config.ini\"\n","\n","# Install requirements, run mask rcnn setup, and load tensorboard\n","%cd \"/content/drive/My Drive/batch-mask/code/Mask_RCNN\"\n","!pip3 install -r requirements.txt\n","!python3 setup.py install\n","%load_ext tensorboard\n","%matplotlib inline"],"metadata":{"id":"YcBt2aJfhatl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77toS72brccI"},"source":["Source Repository: https://github.com/akTwelve/Mask_RCNN.git"]},{"cell_type":"markdown","source":["# Custom Dataset"],"metadata":{"id":"exdz_oybkyK_"}},{"cell_type":"markdown","metadata":{"id":"cx38G-kY4kzK"},"source":["Creates A Session Folder For Custom Datasets"]},{"cell_type":"code","metadata":{"id":"FSSgpOhegtCR"},"source":["import os\n","\n","log_dir=\"/content/drive/MyDrive/batch-mask/data/sessions/example_session\"\n","if (not os.path.exists(log_dir)):\n","  os.makedirs(log_dir)\n","\n","dataset_dir=\"/content/drive/MyDrive/batch-mask/data/datasets/\"\n","\n","with open(log_dir + \"/config.ini\", \"w\") as config_file:\n","  config_file.write(\"[DEFAULT]\\n\")\n","\n","  config_file.write(\"TILE_SIZE = 512\\n\")\n","  config_file.write(\"OVERLAP = 100\\n\")\n","  config_file.write(\"LEARNING_RATE = 0.0001\\n\")\n","  config_file.write(\"TRAIN_TO_VAL = 0.9\\n\")\n","  config_file.write(\"NUM_EPOCHS = 20\\n\")\n","  config_file.write(\"TRAINING_STEPS = 450\\n\")\n","  config_file.write(\"VALIDATION_STEPS = 50\\n\")\n","  config_file.write(\"VAL_SAMPLES_PER_IMAGE = 30\\n\")\n","  config_file.write(\"AUGMENT = False\\n\")\n","  config_file.write(\"DISPLAY_TILES = False\\n\")\n","  config_file.write(\"GREY_STD_RADIUS = 16.6085\\n\")\n","  config_file.write(\"CALCULATE_SCALE_BAR = False\\n\")\n","  config_file.write(\"\\n\")\n","\n","  config_file.write(\"[PATHS]\\n\")\n","  config_file.write(\"DIRECTORY : \" + log_dir + \"\\n\")\n","  config_file.write(\"DATA_SETS : \" + dataset_dir + \"\\n\")\n","  config_file.write(\"TRAIN_VAL_SETS : %(DATA_SETS)s/train_val_sets\\n\")\n","  config_file.write(\"TRAIN_WEIGHTS : coco\\n\")\n","  config_file.write(\"IMAGE_POOL : all\\n\")\n","  config_file.write(\"TEST_SET : %(DATA_SETS)s/test_set_std\\n\")\n","  config_file.write(\"TEST_WEIGHTS : coco\\n\")\n","  if(not os.path.exists(log_dir + \"/weights\")):\n","    os.mkdir(log_dir + \"/weights\")\n","  config_file.write(\"SAVE_WEIGHTS : %(DIRECTORY)s/weights\\n\")\n","  if(not os.path.exists(log_dir + \"/output masks splashes\")):\n","    os.mkdir(log_dir + \"/output masks splashes\")\n","  config_file.write(\"SAVE_SPLASHES : %(DIRECTORY)s/output masks splashes\\n\")\n","  if(not os.path.exists(log_dir + \"/output masks json\")):\n","    os.mkdir(log_dir + \"/output masks json\")\n","  config_file.write(\"SAVE_MASKS_JSON : %(DIRECTORY)s/output masks json\\n\")\n","  if(not os.path.exists(log_dir + \"/output masks binary\")):\n","    os.mkdir(log_dir + \"/output masks binary\")\n","  config_file.write(\"SAVE_MASKS_BINARY : %(DIRECTORY)s/output masks binary\\n\")\n","  with open(log_dir + \"/time_stamps.txt\",\"w+\") as time_stamp:\n","    config_file.write(\"TIME_STAMP : %(DIRECTORY)s/time_stamps.txt\\n\")\n","  config_file.write(\"META_DATA : None\\n\")\n","  config_file.write(\"\\n\")\n","\n","  config_file.write(\"[SCALE BAR]\\n\")\n","  config_file.write(\"default : 550\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NfZYYk70j-p"},"source":["# Code"]},{"cell_type":"markdown","metadata":{"id":"S5V9QTtz_xCW"},"source":["Run to compile"]},{"cell_type":"code","metadata":{"id":"m_oZuXD4-n5u","executionInfo":{"status":"ok","timestamp":1652306078096,"user_tz":240,"elapsed":13616,"user":{"displayName":"Timothy Renney","userId":"06599028348428035797"}}},"source":["############################################################\n","#  IMPORTS\n","############################################################\n","import cv2 as cv2\n","import numpy as np\n","import os.path\n","import sys\n","import random\n","import math\n","import glob\n","import csv\n","import matplotlib.pyplot as plt\n","import json\n","import os\n","import configparser\n","import skimage\n","from skimage import draw\n","from skimage.transform import resize\n","from sklearn.model_selection import train_test_split\n","from sys import exit\n","from datetime import datetime\n","from mrcnn.config import Config\n","from mrcnn import model as modellib, utils\n","import rawpy\n","\n","############################################################\n","#  GLOBAL VARIABLES\n","############################################################\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"../../\")\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)\n","\n","# Path to trained weights file\n","COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","\n","cfg = configparser.ConfigParser()\n","meta_data = {}\n","IMAGE_TYPES = ['.jpg', '.JPG', '.png', '.PNG', '.nef', '.NEF']\n","\n","############################################################\n","#  CONFIG FILE READER\n","############################################################\n","\n","def import_config(file):\n","  \"\"\" Opens config file and assigns it to cfg.\n","  Opens meta file and assigns it to meta_data\n","  file: config file path\n","  \"\"\"\n","  global cfg\n","  if (not os.path.exists(file)):\n","    print(\"Can not find \" + file)\n","    print(\"Double check that the config file exists and that the config path variable (located in the setup section) is correct.\")\n","    exit(1)\n","  cfg.read(file)\n","  if(not cfg['DEFAULT']):\n","    print(\"DEFAULT is not a section in the config file.\")\n","    print(\"This sections should contain parameters that can be adjusted for training the nueral network.\")\n","    print(\"Double check that this section exists in the config file.\")\n","    exit(1)\n","  if(not cfg['PATHS']):\n","    print(\"PATHS is not a section in the config file.\")\n","    print(\"This sections should contain paths for weight files and dataset.\")\n","    print(\"Double check that this section exists in the config file.\")\n","    exit(1)\n","  global meta_data\n","  if(not (cfg_path('META_DATA') == \"None\")):\n","    with open(cfg_path('META_DATA'), 'r') as csv_file:\n","      csv_reader = csv.reader(csv_file, delimiter=',')\n","      for row in csv_reader:\n","        meta_data[row[0]] = row[1:]\n","\n","def cfg_int(key):\n","  \"\"\" Finds integer from config file\n","  key: key for the integer value\n","\n","  Returns integer from config file if key exists. Otherwise report the error.\n","  \"\"\"\n","  if key in cfg['DEFAULT']:\n","    return int(cfg['DEFAULT'][key])\n","  else:\n","    print(\"The \" + key + \"is not in the DEFAULT section of the config file or does not exists.\")\n","    print(\"Double check that the name of the key is spelled correctly in the config file and that the key exists in the DEFAULT section.\")\n","    exit(1)\n","\n","def cfg_float(key):\n","  \"\"\" Finds float from config file\n","  key: key for the float value\n","\n","  Returns float from config file if key exists. Otherwise report the error.\n","  \"\"\"\n","  if key in cfg['DEFAULT']:\n","    return float(cfg['DEFAULT'][key])\n","  else:\n","    print(\"The \" + key + \"is not in the DEFAULT section of the config file or does not exists.\")\n","    print(\"Double check that the name of the key is spelled correctly in the config file and that the key exists in the DEFAULT section.\")\n","    exit(1)\n","\n","def cfg_bool(key):\n","  \"\"\" Finds bool from config file\n","  key: key for the bool value\n","\n","  Returns bool from config file if key exists. Otherwise report the error.\n","  \"\"\"\n","  if key in cfg['DEFAULT']:\n","    return (cfg['DEFAULT'][key] == \"True\")\n","  else:\n","    print(\"The \" + key + \"is not in the DEFAULT section of the config file or does not exists.\")\n","    print(\"Double check that the name of the key is spelled correctly in the config file and that the key exists in the DEFAULT section.\")\n","    exit(1)\n","\n","def cfg_path(key):\n","  \"\"\" Finds path from config file\n","  key: key for the path\n","\n","  Returns path from config file if key exists. Otherwise report the error.\n","  \"\"\"\n","  if key in cfg['PATHS']:\n","    return cfg['PATHS'][key]\n","  else:\n","    print(\"The \" + key + \"is not in the PATHS section of the config file or does not exists.\")\n","    print(\"Double check that the name of the key is spelled correctly in the config file and that the key exists in the PATHS section.\")\n","    exit(1)\n","\n","def cfg_scale_bar(key):\n","  \"\"\" Finds min radius for the scale bar algorithm from config file\n","  key: name of the image\n","\n","  Returns value from config file if key exists. Otherwise returns the default specified in the config file.\n","  If the default value doesn't exists, report an error\n","  \"\"\"\n","  if 'default' in cfg['SCALE BAR']:\n","    if key in cfg['SCALE BAR']:\n","      return int(cfg['SCALE BAR'][key])\n","    else:\n","      return int(cfg['SCALE BAR']['default'])\n","\n","  else:\n","    print(\"The SCALE section of the config file does not contain a default value.\")\n","    print(\"Double check that the name of the 'default' key is spelled correctly in the config file and that the 'default' key exists in the SCALE BAR section.\")\n","    exit(1)\n","\n","############################################################\n","#  UTILITY FUNTIONS\n","############################################################\n","\n","def read_img(path):\n","  \"\"\" Read in image in numpy format either using skimage (jpg, png) or rawpy (nef)\n","\n","  path: path to the image\n","\n","  Returns the image in numpy format.\n","  \"\"\"\n","  image_type = os.path.basename(path).split(\".\")[-1]\n","  if image_type == \"nef\" or image_type == \"NEF\":\n","    raw = rawpy.imread(path)\n","    image = raw.postprocess(no_auto_bright=True, use_auto_wb=False, gamma=None)\n","  else:\n","    image = skimage.io.imread(path)\n","  return image\n","\n","def display_img(image, cmap=None):\n","  \"\"\" Displays an image in the terminal\n","  image: RGB image [height, width, 3]\n","  \"\"\"\n","  fig = plt.figure(figsize = (12,12))\n","  plt.axis(False)\n","  ax = fig.add_subplot(111)\n","  ax.imshow(image, cmap)\n","\n","def preproccess_image(image):\n","  \"\"\" Add gaussian blur to an image using it for training or inference\n","  image: RGB image [height, width, 3]\n","\n","  Returns preprocessed image\n","  \"\"\"\n","  image = cv2.GaussianBlur(image, (5, 5), 0)\n","\n","  return image\n","\n","def clamp(x, x_min, x_max):\n","  # clamps the x value based of x_min and x_max\n","  return max(x_min, min(x_max, x))\n","\n","def random_brightness(image, low, high):\n","  \"\"\" Randomly adjust the brightness of an image\n","  low: (float) lowest possible change\n","  high: (float) highest possible change\n","\n","  Returns image with the adjusted brighness [height, width, 3]\n","  \"\"\"\n","  value = random.uniform(low, high)\n","  hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","  hsv = np.array(hsv, dtype = np.float64)\n","  hsv[:,:,1] = hsv[:,:,1]*value\n","  hsv[:,:,1][hsv[:,:,1]>255]  = 255\n","  hsv[:,:,2] = hsv[:,:,2]*value \n","  hsv[:,:,2][hsv[:,:,2]>255]  = 255\n","  hsv = np.array(hsv, dtype = np.uint8)\n","  image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","  return image\n","\n","def random_color_shift(image, value):\n","  \"\"\" Randomly shifts all color channels by a random value\n","  value: (float) the random shift ranges from -value to value\n","\n","  Returns image with the adjusted color shift [height, width, 3]\n","  \"\"\"\n","  value = int(random.uniform(-value, value))\n","  image = image + value\n","  image[:,:,:][image[:,:,:]>255]  = 255\n","  image[:,:,:][image[:,:,:]<0]  = 0\n","  image = image.astype(np.uint8)\n","  return image\n","\n","def augment_image(image):\n","  \"\"\" Augments an image with a random brightness and color shift\n","  \"\"\"\n","  image = random_brightness(image, 0.5, 3)\n","  image = random_color_shift(image, 60)\n","  return image\n","\n","def get_image_path(images_path, label_path):\n","  \"\"\" Images in the snake dataset end with .JPG or .jpg.\n","  This will automatically return the correct path based on the label path.\n","  images_path: path to folder containing all images in the dataset\n","  label_path: path to the corresponding label file\n","\n","  Returns correct image path\n","  \"\"\"\n","  name = os.path.join(images_path, os.path.basename(label_path)[:-5])\n","  for image_type in IMAGE_TYPES:\n","    if os.path.exists(name + image_type):\n","      return name + image_type\n","  print(\"Could not find image path for : \" + os.path.basename(label_path)[:-5])\n","  print(\"images must be ones of these types: \" + IMAGE_TYPES)\n","  exit(1)\n","\n","def color_splash(image, mask):\n","  \"\"\" Apply color splash effect.\n","  image: RGB image [height, width, 3]\n","  mask: instance segmentation mask [height, width, instance count]\n","\n","  Returns result image.\n","  \"\"\"\n","  # Make a grayscale copy of the image. The grayscale copy still\n","  # has 3 RGB channels, though.\n","  gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255 * [0, 0, 1.5]\n","  # Copy color pixels from the original color image where mask is set\n","  if mask.shape[-1] > 0:\n","    # We're treating all instances as one, so collapse the mask into one layer\n","    mask = (np.sum(mask, -1, keepdims=True) >= 1)\n","    splash = np.where(mask, image, gray).astype(np.uint8)\n","  else:\n","    splash = gray.astype(np.uint8)\n","  return splash\n","\n","def find_contours(mask):\n","  \"\"\" Returns the contours from a brinary mask. \n","  Useful for storing masks in smaller files.\n","  mask: binray segmentation mask [height, width, 1]\n","\n","  Returns contours.\n","  \"\"\"\n","  black = np.zeros(mask.shape, np.float32)\n","  white = np.ones(mask.shape, np.float32) * 255\n","  new_mask = np.where((np.sum(mask, -1, keepdims=True) >= 1), white, black)\n","  gray = cv2.cvtColor(new_mask, cv2.COLOR_BGR2GRAY)\n","  _, threshold = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY)\n","  contours, _ = cv2.findContours(threshold.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","  return contours\n","\n","def find_max_contour(contours):\n","  \"\"\" Returns the contour with the largest area from a list of contours. \n","  contours: contours returned from the find_countours() function\n","\n","  Returns max area contour.\n","  \"\"\"\n","  max_cnt = 0\n","  max_area = 0\n","  for i in range(len(contours)):\n","    cnt = contours[i]\n","    area = cv2.contourArea(cnt)\n","    if max_area < area:\n","      max_area = area\n","      max_cnt = i\n","  return max_cnt, max_area\n","\n","def contour_to_roi(cnt):\n","  \"\"\" Converts a contour to the json file format.\n","  cnt: the contour to be converted.\n","\n","  Returns roi dictionary.\n","  \"\"\"\n","  roi = {}\n","  roi['type'] = 'polygon'\n","  roi['count'] = len(cnt)\n","  roi['x'] = []\n","  roi['y'] = []\n","  for pair in cnt:\n","    roi['x'].append(int(pair[0][0]))\n","    roi['y'].append(int(pair[0][1]))\n","  return roi\n","\n","def draw_roi_polygon(mask, roi, fill = True):\n","  \"\"\" Draws a polygon on a mask based on the roi\n","  mask: A bool array of shape [height, width, 1].\n","  roi: roi dictionary.\n","  fill: if true, fills the polygon with true, otherwise it fills it with false.\n","  \"\"\"\n","  rr, cc = skimage.draw.polygon(roi['y'], roi['x'], (mask.shape[0], mask.shape[1]))\n","  mask[rr, cc, 0] = fill\n","\n","def draw_roi_composite(mask, roi):\n","  \"\"\" Draws a composite roi on a mask.\n","  This only works for coiled shaped rois like snakes.\n","  mask: A bool array of shape [height, width, 1].\n","  roi: roi dictionary.\n","  \"\"\"\n","  if roi['count'] > 0:\n","    draw_roi_polygon(mask, roi['rois'][-1], fill = True)\n","    for i in range(roi['count'] - 1):\n","      draw_roi_polygon(mask, roi['rois'][i], fill = False)\n","\n","############################################################\n","#  UTILITY CLASSES\n","############################################################\n","\n","class dataitem():\n","\n","  def __init__(self, label_path, image_path):\n","    \"\"\" Loads the image file\n","    If a roi file already exists, this will load it.\n","    Otherwise it creates a new roi dictionary.\n","    label_path: path to either load or save the roi.\n","    image_path: path to image.\n","    \"\"\"\n","    self.label_path = label_path\n","    self.image_path = image_path\n","    image = read_img(image_path)\n","    self.height, self.width, self.depth = image.shape\n","    if os.path.exists(label_path):\n","      with open(label_path,'r') as file:\n","        read = file.read()\n","        self.data = json.loads(read)\n","    else:\n","      self.data = {}\n","      self.data['rois'] = []\n","      self.data['classes'] = []\n","      self.data['count'] = 0\n","\n","  def get_mask(self):\n","    \"\"\" Creates a binary mask based on the first composite or polygon roi in the roi dictionary.\n","\n","    Returns a binary mask [height, width, 1]\n","    \"\"\"\n","    mask = np.zeros((self.height, self.width, 1), dtype=np.bool)\n","    for i in range(self.data['count']):\n","      roi = self.data['rois'][i]\n","      if roi['type'] == 'composite':\n","        draw_roi_composite(mask, roi)\n","        return mask\n","      elif roi['type'] == 'polygon':\n","        draw_roi_polygon(mask, roi)\n","        return mask\n","\n","  def cv2_image(self):\n","    \"\"\" Loads the image as a cv2 image\n","\n","    Returns an image [height, width, 3]\n","    \"\"\"\n","    return cv2.imread(self.image_path)\n","\n","  def sk_image(self):\n","    \"\"\" Loads the image as a sk image\n","\n","    Returns an image [height, width, 3]\n","    \"\"\"\n","    return read_img(self.image_path)\n","  \n","  def display_mask(self):\n","    \"\"\" Applies a color splash to the image based on the mask and displays it to the console.\n","\n","    Returns the color splashed image [height, width, 3]\n","    \"\"\"\n","    output = color_splash(self.sk_image(), self.get_mask())\n","    display_img(output)\n","    return output\n","\n","  def display_scalebar(self):\n","    \"\"\" Applies a line roi to the image based on the second roi in the roi dictionary.\n","    Displays the image to the console.\n","\n","    Returns image with the scalebar [height, width, 3]\n","    \"\"\"\n","    output = self.cv2_image()\n","    for i in range(self.data['count']):\n","      if(\"Scale Bar:\" in self.data['classes'][i]):\n","        height, width, depth = output.shape\n","        roi = self.data['rois'][i]\n","        name = self.data['classes'][i]\n","        output = cv2.line(output, (roi['x1'], roi['y1']), (roi['x2'], roi['y2']), (0, 255, 0), 9)\n","        print(name)\n","    \n","        display_img(output)\n","        return output\n","\n","    print(\"Scale bar not found\")\n","    return output\n","\n","  def set_mask(self, mask):\n","    \"\"\" Updates the first roi in the roi dictionary to be reflective of the mask.\n","    mask: binary mask [height, width, 1]\n","    \"\"\"\n","    class_name = os.path.basename(self.image_path)\n","    data_file = os.path.basename(self.label_path)\n","\n","    if data_file in meta_data:\n","      class_name = \"\"\n","      for i in range(1, len(meta_data[data_file]) - 1):\n","        class_name += meta_data[data_file][i] + \"_\"\n","      class_name += meta_data[data_file][len(meta_data[data_file]) - 1]\n","    \n","    class_index = -1\n","    for i in range(self.data['count']):\n","      if self.data['classes'][i] == class_name:\n","        class_index = i\n","\n","    if(class_index == -1):\n","      self.data['classes'].append(class_name)\n","      self.data['rois'].append({})\n","      self.data['count'] = len(self.data['classes'])\n","      class_index = self.data['count'] - 1\n","\n","\n","    contours = find_contours(mask)\n","    max_cnt, max_area = find_max_contour(contours)\n","\n","    self.data['rois'][class_index]['type'] = 'composite'\n","    self.data['rois'][class_index]['count'] = len(contours)\n","    self.data['rois'][class_index]['rois'] = []\n","    if(len(contours) > 0):\n","      for i in range(len(contours)):\n","        if not i == max_cnt:\n","          cnt = contours[i]\n","          roi = contour_to_roi(cnt)\n","          self.data['rois'][class_index]['rois'].append(contour_to_roi(cnt))\n","\n","      roi = contour_to_roi(contours[max_cnt])\n","      self.data['rois'][class_index]['rois'].append(roi)\n","  \n","  def calculate_scalebar(self, maxRadius = 550):\n","    \"\"\" Calculates a scalebar by finding the grey color standard \n","    using the Hough Circles algorithm.\n","    Saves the scalebar to the second roi in the roi dictionary.\n","    maxRadius: (integer) some images require a larger or smaller max radius\n","    depending on how zoomed in the camera is.\n","    \"\"\"\n","    scale_bar_index = -1\n","    for i in range(self.data['count']):\n","      if(\"Scale Bar:\" in self.data['classes'][i]):\n","        scale_bar_index = i\n","    \n","    if scale_bar_index == -1:\n","      self.data['classes'].append(\"\")\n","      self.data['rois'].append({})\n","      self.data['count'] = len(self.data['classes'])\n","      scale_bar_index = self.data['count'] - 1\n","\n","    scale = 4\n","    image = cv2.resize(self.cv2_image(), (int(self.width/scale), int(self.height/scale)))\n","    output = image.copy()\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    # finds possible circles in the image\n","    circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 1000, param1 = 650, param2 = 1, minRadius = 15, maxRadius = int(maxRadius/scale))\n","     \n","    # if there are no circles detected, in form the user\n","    if circles is None:\n","      print(\"no circle detected\")\n","    else:\n","      circles = np.round(circles[0, :]).astype(\"int\")\n","\n","      # x and y are the center of the circle, r is the radius\n","      x, y, r = circles[0]\n","\n","      # scale up the coordinates\n","      x = x*scale\n","      y = y*scale\n","      r = r*scale\n","      GREY_STD_RADIUS = cfg_float('GREY_STD_RADIUS') # radius of circle in mm\n","\n","      # saves the scale bar to the second roi\n","      self.data['classes'][1] = (\"Scale Bar:\" + str(r) + \":\" + str(GREY_STD_RADIUS))\n","      self.data['rois'][1]['type'] = 'line'\n","      self.data['rois'][1]['x1'] = int(x)\n","      self.data['rois'][1]['y1'] = int(y)\n","      self.data['rois'][1]['x2'] = int(x + r)\n","      self.data['rois'][1]['y2'] = int(y)\n","    \n","  def save_data(self):\n","    \"\"\" Saves the current roi dictionary based on the label path\n","    \"\"\"\n","    with open(self.label_path,'w') as file:\n","      file.write(json.dumps(self.data))\n","\n","class image_pool_class():\n","  def __init__(self):\n","    \"\"\" Loads the images and labels for training \n","    based on the paths in the config file.\n","    \"\"\"\n","    \n","    self.subpools = {}\n","    self.subpools['all'] = {}\n","    self.subpools['all']['paths'] = []\n","    self.subpools['all']['train'] = []\n","    self.subpools['all']['val'] = []\n","\n","    for root, dirs, files in os.walk(cfg_path('TRAIN_VAL_SETS'), topdown=False):\n","      if(root != cfg_path('TRAIN_VAL_SETS')):\n","        subset = os.path.basename(root)\n","        self.subpools[subset] = {} \n","        self.subpools[subset]['paths'] = glob.glob(root + '/*.json')\n","        self.subpools[subset]['train'], self.subpools[subset]['val'] = train_test_split(self.subpools[subset]['paths'], train_size=cfg_float('TRAIN_TO_VAL'), random_state=0)\n","        self.subpools['all']['paths'] += self.subpools[subset]['paths']\n","        self.subpools['all']['train'] += self.subpools[subset]['train']\n","        self.subpools['all']['val'] += self.subpools[subset]['val']\n","        \n","\n","    self.train_pool = self.subpools[cfg_path('IMAGE_POOL')]['train']\n","    self.val_pool = self.subpools[cfg_path('IMAGE_POOL')]['val']\n","\n","    self.datum = {}\n","\n","    for path in self.subpools['all']['paths']:\n","      self.datum[path] = dataitem(path, get_image_path(os.path.dirname(path), path))\n","\n","class SnakeConfig(Config):\n","    \"\"\"Configuration for training on the snake dataset.\n","    Derives from the base Config class and overrides some values.\n","    \"\"\"\n","\n","    def __init__(self):\n","      # Give the configuration a recognizable name\n","      self.NAME = \"snake\"\n","\n","      self.GPU_COUNT = 1\n","      self.IMAGES_PER_GPU = 1\n","\n","      # Number of classes (including background)\n","      self.NUM_CLASSES = 2\n","\n","      batch_size = self.GPU_COUNT * self.IMAGES_PER_GPU\n","      self.STEPS_PER_EPOCH = cfg_int('TRAINING_STEPS') // batch_size\n","      self.VALIDATION_STEPS = cfg_int('VALIDATION_STEPS') // batch_size\n","      \n","      self.IMAGE_MIN_DIM = cfg_int('TILE_SIZE')\n","      self.IMAGE_MAX_DIM = cfg_int('TILE_SIZE')\n","\n","      # Since there is only one class, we can set min confidence to zero\n","      self.DETECTION_MIN_CONFIDENCE = 0.0\n","      self.LEARNING_RATE = cfg_float('LEARNING_RATE')\n","      Config.__init__(self)\n","\n","\n","\n","############################################################\n","#  Dataset\n","############################################################\n","\n","class SnakeDataset(utils.Dataset):\n","    \n","  def load_snake(self, subset, image_pool):\n","\n","    \"\"\"Load a subset of the Snake dataset.\n","    subset: Subset to load: train or val\n","    image_pool: image pool to load images and labels from\n","    \"\"\"\n","    self.image_pool = image_pool\n","    self.subset = subset\n","    self.displayed = 0\n","    # Add classes.\n","    self.add_class(\"snake\", 1, \"snake\")\n","\n","    if self.subset == 'val':\n","      data_paths = image_pool.val_pool\n","    else:\n","      data_paths = image_pool.train_pool\n","\n","    random.seed(0)\n","    \n","    for data_path in data_paths:\n","      data = image_pool.datum[data_path]\n","      \n","      if self.subset == 'val':\n","        for i in range(cfg_int('VAL_SAMPLES_PER_IMAGE')):\n","          x = random.randint(0, data.width + cfg_int('TILE_SIZE'))\n","          y = random.randint(0, data.height + cfg_int('TILE_SIZE'))\n","\n","          self.add_image(\n","            \"snake\",\n","            image_id=os.path.basename(data.image_path) + '_' + str(i),  # use file name as a unique image id\n","            tile=i, path=data.label_path, x=x, y=y)\n","      else:\n","        self.add_image(\n","          \"snake\",\n","          image_id=os.path.basename(data.image_path),  # use file name as a unique image id\n","          path=data.label_path)\n","\n","  \n","  def load_mask(self, image_id):\n","      \"\"\"Generate instance masks for an image.\n","      Returns:\n","      masks: A bool array of shape [height, width, instance count] with\n","          one mask per instance.\n","      class_ids: a 1D array of class IDs of the instance masks.\n","\n","      Returns the generated mask and class ids.\n","      \"\"\"\n","      # If not a snake dataset image, delegate to parent class.\n","      image_info = self.image_info[image_id]\n","      if image_info[\"source\"] != \"snake\":\n","        return super(self.__class__, self).load_mask(image_id)\n","\n","      # Convert polygons to a bitmap mask of shape\n","      # [height, width, instance_count]\n","      info = self.image_info[image_id]\n","      data_path = info['path']\n","      mask = self.image_pool.datum[data_path].get_mask()\n","      class_ids = np.ones([mask.shape[-1]], dtype=np.int32)\n","\n","      # Return mask, and array of class IDs of each instance.\n","      tile_size = cfg_int('TILE_SIZE')\n","      mask = np.pad(mask, ((tile_size, tile_size), (tile_size, tile_size), (0, 0)))\n","      mask = mask[info[\"y\"] : info[\"y\"] + tile_size, info[\"x\"] : info[\"x\"] + tile_size]\n","      return mask, class_ids\n","\n","        \n","  def load_image(self, image_id):\n","    \"\"\"Generate an image from the specs of the given image ID.\n","    Typically this function loads the image from a file, but\n","    in this case it generates the image on the fly from the\n","    specs in image_info.\n","\n","    Returns the generated image.\n","    \"\"\"\n","    info = self.image_info[image_id]\n","    data = self.image_pool.datum[info['path']]\n","    image = data.sk_image()\n","    tile_size = cfg_int('TILE_SIZE')\n","    image = np.pad(image, ((tile_size, tile_size), (tile_size, tile_size), (0, 0)))\n","    # randomly choose tile if training\n","    if self.subset == 'train':\n","      info[\"x\"] = random.randint(0, data.width + tile_size)\n","      info[\"y\"] = random.randint(0, data.height + tile_size)\n","    \n","    image = image[info[\"y\"] : info[\"y\"] + tile_size, info[\"x\"] : info[\"x\"] + tile_size]\n","    # augment image if set to true in config file\n","    if cfg_bool('AUGMENT'):\n","      if self.subset == 'train' and random.random() > 0.5:\n","        image = augment_image(image)\n","\n","    # used for trouble shooting\n","    if cfg_bool(\"DISPLAY_TILES\"):\n","      self.show_image(image_id, image)\n","\n","    return image\n","\n","  def image_reference(self, image_id):\n","    \"\"\"Return the path of the image.\"\"\"\n","    info = self.image_info[image_id]\n","    if info[\"source\"] == \"snake\":\n","      return self.image_pool.datum[info[\"path\"]].image_path\n","    else:\n","      super(self.__class__, self).image_reference(image_id)\n","\n","  def show_image(self, image_id, image):\n","    \"\"\"Displays masked image tiles while training for trouble shooting\n","    image_id: ID to use for the image info.\n","    image: the image tile itself\n","    \"\"\"\n","    # can only handle outputing a few images. We set the limit to 30 here.\n","    if self.displayed < 30:\n","      mask ,_ = self.load_mask(image_id)\n","      gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255 * [0, 0, 1.5]\n","      dis_image = np.where(mask, image, gray).astype(np.uint8)\n","      display_img(dis_image)\n","      self.displayed += 1\n","\n","############################################################\n","#  Model\n","############################################################\n","def load_model(weights, logs, mode):\n","  \"\"\" Loads neural network model.\n","  weights: path to weights file\n","  logs: path to logs folder\n","  mode: either \"inference\" or \"training\"\n","  \"\"\" \n","  config = SnakeConfig()\n","  config.display()\n","\n","  model = modellib.MaskRCNN(mode=mode, config=config,\n","                                model_dir=logs)\n","  \n","  # Select weights file to load\n","  if weights.lower() == \"coco\":\n","    weights_path = COCO_WEIGHTS_PATH\n","    # Download weights file\n","    if not os.path.exists(weights_path):\n","      utils.download_trained_weights(weights_path)\n","  elif weights.lower() == \"last\":\n","    # Find last trained weights\n","    weights_path = model.find_last()\n","  elif weights.lower() == \"imagenet\":\n","    # Start from ImageNet trained weights\n","    weights_path = model.get_imagenet_weights()\n","  else:\n","    weights_path = weights\n","\n","\n","  # Load weights\n","  if weights.lower() == \"coco\":\n","    # Exclude the last layers because they require a matching\n","    # number of classes\n","    model.load_weights(weights_path, by_name=True, exclude=[\n","      \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n","      \"mrcnn_bbox\", \"mrcnn_mask\"])\n","  else:\n","    model.load_weights(weights_path, by_name=True)\n","  \n","\n","  print(\"Loading weights \", weights_path)\n","\n","  return config, model\n","\n","############################################################\n","#  Training\n","############################################################\n","\n","def train(config = \"\"):\n","  \"\"\" Trains the neural network model based on the specified config file.\n","  \"\"\"\n","  import_config(config)\n","  weights = cfg_path('TRAIN_WEIGHTS')\n","  logs = cfg_path('SAVE_WEIGHTS')\n","  num_epochs = cfg_int('NUM_EPOCHS')\n","  \n","  image_pool = image_pool_class()\n","\n","  config, model = load_model(weights, logs, \"training\")\n","  \"\"\"Train the model.\"\"\"\n","  # Training dataset.\n","  dataset_train = SnakeDataset()\n","  dataset_train.load_snake('train', image_pool)\n","  dataset_train.prepare()\n","\n","  # Validation dataset\n","  dataset_val = SnakeDataset()\n","  dataset_val.load_snake('val', image_pool)\n","  dataset_val.prepare()\n","\n","  # *** This training schedule is an example. Update to your needs ***\n","  # Since we're using a very small dataset, and starting from\n","  # COCO trained weights, we don't need to train too long. Also,\n","  # no need to train all layers, just the heads should do it.\n","  print(\"Training network heads\")\n","  import time\n","  start_time = time.time()\n","  model.train(dataset_train, dataset_val,\n","              learning_rate=config.LEARNING_RATE,\n","              epochs = num_epochs,\n","              layers='all')\n","  print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","\n","############################################################\n","#  Detection\n","############################################################\n","\n","def detect_mask(image, model):\n","  \"\"\" Subdivides the image and runs each \n","  subdivision on the neural network model.\n","  Postprocesses the image by choosing the contour \n","  with the largest surface area as the correct masked object.\n","  \"\"\"\n","  overlap = cfg_int('OVERLAP')\n","  # reference image. Adds a black border around the original image\n","  ref = np.pad(image, ((overlap, overlap), (overlap, overlap), (0, 0)))\n","  height, width = ref.shape[:2]\n","\n","  # mask starts as black\n","  mask = np.zeros([height,width,3])\n","\n","  tile_size = cfg_int('TILE_SIZE')\n","  num_tiles = 0\n","\n","  row = 0\n","  while row < height:\n","    col = 0\n","    while col < width:\n","      # start with a black tile\n","      tile = np.zeros([tile_size,tile_size,3])\n","\n","      row_min = max(row, 0)\n","      row_max = min(row + tile_size, height)\n","      col_min = max(col, 0)\n","      col_max = min(col + tile_size, width)\n","      row_start = 0\n","      row_end = tile_size\n","      col_start = 0\n","      col_end = tile_size\n","\n","      if row < 0:\n","        row_start = -row\n","      if row + tile_size > height:\n","        row_end = height - row\n","      if col < 0:\n","        col_start = -col\n","      if col + tile_size > width:\n","        col_end = width - col\n","\n","      tile[row_start:row_end, col_start:col_end] = ref[row_min:row_max, col_min:col_max]\n","      tile_mask = model.detect([tile], verbose=1)[0]\n","      tile_mask = (np.sum(tile_mask['masks'], -1, keepdims=True) >= 1)\n","\n","      tile_row_start = max(overlap, row_start)\n","      tile_row_end = min(tile_size - overlap, row_end)\n","      tile_col_start = max(overlap, col_start)\n","      tile_col_end = min(tile_size - overlap, col_end)\n","\n","      mask_row_start = (row + overlap)\n","      mask__row_end = (row + tile_size - overlap)\n","      mask_col_start = (col + overlap)\n","      mask_col_end = (col + tile_size - overlap)\n","\n","      mask[mask_row_start:mask__row_end, mask_col_start:mask_col_end] = tile_mask[tile_row_start:tile_row_end, tile_col_start:tile_col_end]\n","      num_tiles += 1\n","\n","      # since we want the tiles to overlap, we increase by tile_size - 2 * overlap instea of just tile_size\n","      col += tile_size - 2 * overlap\n","    row += tile_size - 2 * overlap\n","\n","  print(\"number of tiles: \" + str(num_tiles))\n","\n","  # remove black border\n","  mask = mask[overlap:height - overlap, overlap:width - overlap]\n","\n","  # postprocessing\n","  contours = find_contours(mask)\n","\n","  final_mask = skimage.img_as_ubyte(np.zeros(image.shape, np.float32))\n","  if len(contours) > 0:\n","    max_cnt, max_area = find_max_contour(contours)\n","    final_mask = cv2.drawContours(final_mask, [contours[max_cnt]], 0, 255, -1)\n","  black = np.zeros(mask.shape, np.float32)\n","  final_mask = np.where(np.invert(np.sum(mask, -1, keepdims=True) >= 1), black, final_mask)\n","\n","  return final_mask\n","\n","def detect(image_path, model, output_type, resume, scalebar):\n","  \"\"\"\n","  Detects the snake in the image using the detect_mask function.\n","  image_path: path to the image\n","  model: Mask rcnn model with loaded weights\n","  output_type: three output types\n","    -\"splash\": applies a blue splash to the background of the image \n","      and saves it to the \"SAVE_SPLASHES\" path in the config file\n","    -\"json\": saves the mask to a json roi file to the \"SAVE_MASKS_JSON\" path in the config file.\n","      This one also calculates the scale bar and adds it to the json file.\n","      We used these files for the ImageJ analysis.\n","      The training labels are also in this format\n","    -\"binary\": saves the mask to a csv file with 0's and 1's representing the binary mask.\n","      This is the most universal output and can be imported easiest into other softwares.\n","  resume: if True, it will only run if the output file doesn't already exist.\n","  \"\"\"\n","  # Run model detection and generate the color splash effect\n","  image_name = os.path.basename(image_path)\n","  image_id = image_name.split('.')[0]\n","  otypes = output_type.split('&')\n","  \n","  binary_path = os.path.join(cfg_path('SAVE_MASKS_BINARY'), image_id + '.csv')\n","  label_path = os.path.join(cfg_path('SAVE_MASKS_JSON'), image_id + '.json')\n","  splash_path = os.path.join(cfg_path('SAVE_SPLASHES'), image_id + '.jpg')\n","\n","  \n","  if resume:\n","    if 'splash' in otypes:\n","      if os.path.exists(splash_path):\n","        otypes.remove('splash')\n","    if 'json' in otypes:\n","      if os.path.exists(label_path):\n","        otypes.remove('json')\n","    if 'binary' in otypes:\n","      if os.path.exists(binary_path):\n","        otypes.remove('binary')\n","\n","\n","  if len(otypes) > 0:\n","    print(\"Running on {}\".format(image_path))\n","    data = dataitem(label_path, image_path)\n","    image = data.sk_image()\n","    mask = detect_mask(image, model)\n","    data.set_mask(mask)\n","\n","    # Save output\n","    if 'splash' in otypes:\n","      splash = data.display_mask()\n","      skimage.io.imsave(splash_path, splash)\n","    if 'json' in otypes:\n","      if scalebar:\n","        data.calculate_scalebar(cfg_scale_bar(image_id))\n","      data.save_data()\n","    if 'binary' in otypes:\n","      with open(binary_path, 'w', encoding='UTF8') as binary_file:\n","        writer = csv.writer(binary_file)\n","        for row in range(data.height):\n","          next_row = []\n","          for col in range(data.width):\n","            if mask[row][col][0] > 0:\n","              next_row.append(1)\n","            else:\n","              next_row.append(0)\n","          writer.writerow(next_row)\n","\n","def detect_batch(config = \"\", output_type=\"splash\", resume=True, scalebar=True):\n","  \"\"\" Calls the detect function for an entire folder of images.\n","  The path to these images are determined by the \"TEST_SET\" path in the config file.\n","  config: config file to load\n","  output_type: three output types\n","    -\"splash\": applies a blue splash to the background of the image \n","      and saves it to the \"SAVE_SPLASHES\" path in the config file\n","    -\"json\": saves the mask to a json roi file to the \"SAVE_MASKS_JSON\" path in the config file.\n","      We used these files for the ImageJ analysis.\n","      The training labels are also in this format\n","    -\"binary\": saves the mask to a csv file with 0's and 1's representing the binary mask.\n","      This is the most universal output and can be imported easiest into other softwares.\n","  resume: if True, it will resume a previous session that was either stopped or crashed.\n","          Otherwise it will start from the first image and overwrite any existing data.\n","  scalebar: if mode is \"json\", then it will calculate the scale bar \n","            and add it to the json file when this is true.\n","  \"\"\"\n","  import_config(config)\n","  weights = cfg_path('TEST_WEIGHTS')\n","  image_folder = cfg_path('TEST_SET')\n","  logs = cfg_path('SAVE_WEIGHTS')\n","\n","  config, model = load_model(weights, logs, \"inference\")\n","\n","  # Image or video?\n","  if image_folder:\n","    for image_type in IMAGE_TYPES:\n","      image_paths = glob.glob(image_folder + \"/*\" + image_type)\n","      # Read image\n","      for image_path in image_paths:\n","        detect(image_path, model, output_type, resume, scalebar)\n","  print(\"Finished!!!\")\n","\n","############################################################\n","#  EVALUATION\n","############################################################\n","\n","def compare_label_to_inference(model, data):\n","  \"\"\" Calculates the intersection over union and intersection over labeled\n","  to compare the labeled mask to the infered mask.\n","  model: Mask RCNN model loaded with weights to use for inference\n","  data: dataitem that contains the labeled mask\n","  \"\"\"\n","  width = data.width\n","  height = data.height\n","  labeled_mask = data.get_mask().reshape(height, width)\n","\n","  model_mask = detect_mask(data.sk_image(), model)\n","  black = np.zeros((height, width), np.bool)\n","  white = np.ones((height, width), np.bool)\n","  model_mask = np.where((np.sum(model_mask, -1, keepdims=True) >= 1).reshape(height, width), white, black)\n","\n","  intersection = np.logical_and(model_mask, labeled_mask)\n","  union = np.logical_or(model_mask, labeled_mask)\n","  iou = np.sum(intersection) / np.sum(union)\n","  iol = np.sum(intersection) / np.sum(labeled_mask)\n","  return iou, iol\n","\n","# Calculate evalutaion metric\n","def eval_metric(config):\n","  \"\"\" Calculates the average intersection over union and intersection over labeled\n","  using the compare_label_to_inference function.\n","  config: config file to load\n","  \"\"\"\n","  import_config(config)\n","  weights = cfg_path('TEST_WEIGHTS')\n","  logs = cfg_path('SAVE_WEIGHTS')\n","  image_pool = image_pool_class()\n","  config, model = load_model(weights, logs, \"inference\")\n","\n","  ious = {}\n","  iols = {}\n","\n","  for subset in image_pool.subpools:\n","    ious[subset] = []\n","    iols[subset] = []\n","    # run for all subpool images\n","    for data_path in image_pool.subpools[subset]['val']:\n","      iou, iol = compare_label_to_inference(model, image_pool.datum[data_path])\n","\n","      ious[subset].append(iou)\n","      iols[subset].append(iol)\n","\n","  for subset in image_pool.subpools:\n","    # output the results to the console\n","    print(\"Average \" + subset + \" IOU: \" + str(np.mean(np.array(ious[subset]))))\n","    print(\"Average \" + subset + \" IOL: \" + str(np.mean(np.array(iols[subset]))))\n","\n","\n","############################################################\n","#  CHECK DATASET\n","############################################################\n","\n","def check(images_path, labels_path, start=0, end=50, mode='json'):\n","  \"\"\" Displays the mask and scale bar from the roi json files.\n","  images_path: path to the folder containing the images.\n","  labels_path: path to the folder containing the roi json files.\n","  \"\"\"\n","  # Colab ram can only handle checking 50 images at once\n","  labels = glob.glob(labels_path + \"/*.json\")[start:end]\n","  for label in labels:\n","    if mode == 'json':\n","      data = dataitem(label, get_image_path(images_path, label))\n","\n","      data.display_mask()\n","      data.display_scalebar()\n","    elif mode == 'binary':\n","      with open(label, 'r') as file:\n","        image = read_img(get_image_path(images_path, label))\n","        height, width, = image.shape\n","        # mask starts as black\n","        mask = np.zeros([height,width,3])\n","        csv_reader = csv.reader(file, delimiter=',')\n","\n","        for row in len(csv_reader):\n","          for col in len(csv_reader[row]):\n","            if csv_reader[row][col] == 1:\n","              mask[row][col] = [255, 255, 255]\n","\n","        image = read_img(get_image_path(images_path, label))\n","        display_img(color_splash(image, mask))\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GiQ-oEj0Ssq"},"source":["# Check Dataset"]},{"cell_type":"markdown","metadata":{"id":"V9tU-KsMnetq"},"source":["Check Dataset Command"]},{"cell_type":"markdown","source":["Can be used for veiwing either a human labeled dataset (the training set) or a nueral network labeled dataset (the inference set). This is useful to make sure a human labeled dataset is being loaded properly or to qualitatively assess a neural network labeled dataset."],"metadata":{"id":"905EXmCYl5AA"}},{"cell_type":"code","metadata":{"id":"0N3TnCx8niOP"},"source":["check(images_path=\"/content/drive/MyDrive/batch-mask/data/datasets/train_val_sets/dorsal_set\", labels_path=\"/content/drive/MyDrive/batch-mask/data/datasets/train_val_sets/dorsal_set\", start=0, end=50, mode=\"json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzO12S2L0Kim"},"source":["#Train"]},{"cell_type":"markdown","metadata":{"id":"qseVGvXzg-V7"},"source":["Training from scratch (weights are saved to the specified weights directory int the config file)"]},{"cell_type":"code","metadata":{"id":"6B9gYHGiXkyh"},"source":["train(config=CONFIG_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HaJE5v2lD_15"},"source":["Run to view loss values"]},{"cell_type":"code","metadata":{"id":"yraKW5Inh72o"},"source":["%tensorboard --logdir=(\"/content/drive/MyDrive/batch-mask/data/sessionsx/all/weights/snake20210712T1704\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QB9aSJEB0OlA"},"source":["# Detect"]},{"cell_type":"markdown","metadata":{"id":"IW97uZaehBp-"},"source":["Detecting Image Command"]},{"cell_type":"code","metadata":{"id":"BFApBwNwhBEp"},"source":["detect_batch(config=CONFIG_PATH, output_type=\"json&splash&binary\", resume=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4XBY2IYZNz25"},"source":["##Get Model Metric##"]},{"cell_type":"code","metadata":{"id":"uYUsRU06NR2n"},"source":["eval_metric(config=CONFIG_PATH)"],"execution_count":null,"outputs":[]}]}